{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7954b93",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141cccb",
   "metadata": {},
   "source": [
    "Data collection is one of the important steps that comes after defining and analysing a problem in a datascience project. Though there are several ways of collecting data, we will focus on \"webscraping\" to collect data for our datascience projects.\n",
    "\n",
    "Building a universal webscraper is neither feasible nor desirable. Every webscraper should be made for a particular situation and should be modified according to the changes in the page to be scraped.\n",
    "\n",
    "But, here we will focus on creating a simple scraper using Beautifulsoup and Requests library that can scrape — by default, links and images — any static webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b68d4f",
   "metadata": {},
   "source": [
    "## Defining the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6881f5",
   "metadata": {},
   "source": [
    "We will make use of OOP in creating this scraper from scratch in Python.\n",
    "Lets list out the functions we require to build this scraper\n",
    "\n",
    "1. A method for parsing the url  \n",
    "    This method should create a custom object that could be used to access any web element in the given page.\n",
    "    \n",
    "2. A method for fetching the page title (not necessary though, but it doesn't hurt either)\n",
    "3. A method for fetching all links from the webpage\n",
    "4. A method for fetching all image links from the webpage\n",
    "5. A method for downloading images from the image links to your current directory.\n",
    "\n",
    "Now that we have listed the core functionalities of this scraper, lets build the functions step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367626c7",
   "metadata": {},
   "source": [
    "## Building the scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853216c",
   "metadata": {},
   "source": [
    "Lets create the class and define our methods. We will call this scraper by the name \"StaticSiteScraper\".\n",
    "\n",
    "```python\n",
    "class StaticSiteScraper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # A method for parsing the url\n",
    "    def url_parse(self):\n",
    "        pass\n",
    "    \n",
    "    # A method for fetching the page title\n",
    "    def get_title(self):\n",
    "        pass\n",
    "    \n",
    "    # A method for fetching all links from the webpage\n",
    "    def get_all_links(self):\n",
    "        pass\n",
    "    \n",
    "    # A method for fetching all image links from the webpage\n",
    "    def get_all_image_links(self):\n",
    "        pass\n",
    "    \n",
    "    # A method for downloading images from the image links\n",
    "    def download_images(self):\n",
    "        pass  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f3add",
   "metadata": {},
   "source": [
    "### Fetching the html page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06318b23",
   "metadata": {},
   "source": [
    "Lets define the function for fetching the html page.\n",
    "\n",
    "We will use requests library to fetch the html content from the url and store it in a variable called data.\n",
    "\n",
    "```python \n",
    "data = requests.get(url).text\n",
    "```\n",
    "\n",
    "Then, we will create a soup object that will parse this html content into readable text format.\n",
    "\n",
    "```python \n",
    "soup = bs(data, parser) \n",
    "```\n",
    "\n",
    "And finally our function will return this soup object.\n",
    "\n",
    "Putting it all together, we have:\n",
    "\n",
    "```python\n",
    "def url_parse():\n",
    "    data = requests.get(url).text\n",
    "    soup = bs(data, parser)\n",
    "    return soup\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a09a9b",
   "metadata": {},
   "source": [
    "The soup object created can be used to access any element within the parsed html content. \n",
    "\n",
    "For example, we can access the title text of the webpage using the following:\n",
    "```python\n",
    "soup.title.text\n",
    "```\n",
    "Similarly, you can use the soup object to access any element you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77bb92",
   "metadata": {},
   "source": [
    "Now that we have our soup object, we can use this object to create methods for fetching all the links and images from the parsed html content.\n",
    "\n",
    "### Fetching all the links\n",
    "\n",
    "We start by defining a list for storing all the links we collect using the method. This list will be the output for the method.\n",
    "\n",
    "\n",
    "We know that links are stored in \\<a\\> tag within an html page. \n",
    "    So, to access the links, we need to access the \\<a\\> tag first which can be done using:\n",
    "    \n",
    "```python\n",
    "   soup.find_all('a')\n",
    "```\n",
    "This will fetch us a list of all the \\<a\\> tags in the parsed html content. Now, we will have to iterate through each of the elements within this list and extract the link from the \"href\" attribute. Then, we will store the collected links in the list that we had defined earlier.\n",
    "\n",
    "```python\n",
    "for tag in a_tags:\n",
    "    items = str(tag).split(\" \")\n",
    "\n",
    "    for item in items:\n",
    "        if 'href' in item:\n",
    "            link_item = item.split(\"=\")[1].split(\"\\\"\")[1]\n",
    "            if 'http' in link_item or 'www' in link_item:\n",
    "                links.append(link_item)\n",
    "```\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "```python\n",
    "def get_all_links():\n",
    "    links=[]\n",
    "    \n",
    "    a_tags = soup.find_all('a')\n",
    "\n",
    "    for tag in a_tags:\n",
    "        items = str(tag).split(\" \")\n",
    "        for item in items:\n",
    "            if 'href' in item:\n",
    "                link_item = item.split(\"=\")[1].split(\"\\\"\")[1]\n",
    "                if 'http' in link_item or 'www' in link_item:\n",
    "                    links.append(link_item)\n",
    "    return links\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386daa68",
   "metadata": {},
   "source": [
    "### Fetching all image links\n",
    "\n",
    "This step is exactly similar to the step above where we had collected all the links from the parsed content. \n",
    "\n",
    "The only difference here is that instead of \\<a\\> tag, we use\\<img\\> tag here. And also, instead of \"href\", we use \"src\" attribute for accessing the image links.\n",
    "\n",
    "```python\n",
    "def get_all_image_links():\n",
    "    img_links=[]\n",
    "\n",
    "    img_tags = soup.find_all('img')\n",
    "    for tag in img_tags:\n",
    "        items = str(tag).split(\" \")\n",
    "        for item in items:\n",
    "            if 'src' in item:\n",
    "                link_item = item.split(\"=\")[1].split(\"\\\"\")[1]\n",
    "                if 'http' in link_item or 'www' in link_item:\n",
    "                    img_links.append(link_item)\n",
    "    return img_links\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7dcc19",
   "metadata": {},
   "source": [
    "### Downloading the images\n",
    "\n",
    "Now, we have all the image links with us, we will now proceed to download all the images from these links and store it in our local directory.  We will pass the image list as a parameter to this method.\n",
    "\n",
    "We will start by creating a date_ variable that stores current day date in dd-mm-yyyy format.\n",
    "\n",
    "```python\n",
    "day = datetime.date.today().day\n",
    "month = datetime.date.today().month\n",
    "year = datetime.date.today().year\n",
    "\n",
    "date_ = str(day)+\"-\"+str(month)+\"-\"+str(year)+\"_\"\n",
    "```\n",
    "Before creating a new directory to store the images, we will check for the presence of \"images\" directory in our current working directory. If not, we will create a new \"images\" directory.\n",
    "Inside \"images\" directory, we will create a new directory with current day date as its name to store the images.\n",
    "\n",
    "```python\n",
    "    if 'images' not in os.listdir():\n",
    "        os.mkdir(\"images\")\n",
    "        os.chdir(\"images\")\n",
    "\n",
    "    os.mkdir(date_)\n",
    "    os.chdir(date_)\n",
    "```\n",
    "Now, we will initiate an image number variable to name the images. We can access the \\<alt\\> tag to fetch the image name from the url, but sometimes, images won't have an alt name. So, to keep image names uniform, we will number the images.\n",
    "\n",
    "```python\n",
    "img_no = 1\n",
    "```\n",
    "Next, we will iterate through the image links, fetch the images and store it in our local directory.\n",
    "\n",
    "```python\n",
    "for link in image_list:\n",
    "    img_response = requests.get(link)\n",
    "\n",
    "    img_format = link.split(\".\")[-1]\n",
    "\n",
    "    filename = \"img\" + str(img_no) + \".\" + img_format\n",
    "\n",
    "    with open(filename, \"wb+\") as f:\n",
    "        f.write(img_response.content)\n",
    "    img_no += 1\n",
    "```\n",
    "\n",
    "Putting it all together, we have:\n",
    "\n",
    "```python\n",
    "def download_images(image_list):\n",
    "    day = datetime.date.today().day\n",
    "    month = datetime.date.today().month\n",
    "    year = datetime.date.today().year\n",
    "\n",
    "    date_ = str(day)+\"-\"+str(month)+\"-\"+str(year)+\"_\"\n",
    "\n",
    "    if 'images' not in os.listdir():\n",
    "        os.mkdir(\"images\")\n",
    "        os.chdir(\"images\")\n",
    "\n",
    "    os.mkdir(date_)\n",
    "    os.chdir(date_)\n",
    "\n",
    "    img_no = 1\n",
    "\n",
    "    for link in image_list:\n",
    "        img_response = requests.get(link)\n",
    "\n",
    "        img_format = link.split(\".\")[-1]\n",
    "\n",
    "        filename = \"img\" + str(img_no) + \".\" + img_format\n",
    "\n",
    "        with open(filename, \"wb+\") as f:\n",
    "            f.write(img_response.content)\n",
    "        img_no += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4322299",
   "metadata": {},
   "source": [
    "Now that we have defined all our methods, we will put it all together to create our StaticSiteScraper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e428f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticSiteScraper:\n",
    "    '''\n",
    "        Author: Retin P Kumar\n",
    "        Contact: retinpkumar@gmail.com\n",
    "        \n",
    "        Methods\n",
    "        =======\n",
    "        \n",
    "        url_parse\n",
    "        ---------\n",
    "        Returns the parsed html content of given url\n",
    "        \n",
    "        get_title\n",
    "        ---------\n",
    "        Returns the title of the given webpage\n",
    "        \n",
    "        get_all_links\n",
    "        -------------\n",
    "        Returns a list of all the links in the given url\n",
    "        \n",
    "        get_all_image_links\n",
    "        -------------------\n",
    "        Returns a list of all the links of images in the given url\n",
    "        \n",
    "        download_images\n",
    "        ---------------\n",
    "        Checks for an \"images\" directory within the current working directory.\n",
    "        If not found, creates an \"images\" directory in the current working \n",
    "        directory. Saves all the images from the links returned by get_all_image_links\n",
    "        method.       \n",
    "    '''\n",
    "    # importing libraries\n",
    "    import os\n",
    "    import datetime\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    \n",
    "    def __init__(self, url: str, parser: str='html.parser'):\n",
    "        '''\n",
    "            Parameters\n",
    "            ----------\n",
    "            url: url of webpage\n",
    "            parser: object for parsing text files\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            self: object\n",
    "        '''\n",
    "        self.url = url\n",
    "        self.parser = parser\n",
    "    \n",
    "    def url_parse(self): \n",
    "        '''\n",
    "        A method for parsing the url\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            self: object\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            self.soup: parsed web html content\n",
    "        '''\n",
    "        try:\n",
    "            # getting response from webpage\n",
    "            data = self.requests.get(self.url).text\n",
    "            # creating a html parsed object\n",
    "            self.soup = self.bs(data, self.parser)\n",
    "            return self.soup\n",
    "        except Exception as e:\n",
    "            print(\"Url not parsable. \", e)\n",
    "        finally:\n",
    "            # printing a prettified version of parsed html content\n",
    "            print(self.soup.prettify())\n",
    "    \n",
    "    def get_title(self):\n",
    "        '''\n",
    "        A method for fetching the page title\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            self: object\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            title: webpage title text\n",
    "        '''\n",
    "        try:\n",
    "            # fetching the webpage title\n",
    "            title = self.soup.title.text\n",
    "            print(title)\n",
    "        except:\n",
    "            print(\"Cannot find title.\")\n",
    "            \n",
    "    def get_all_links(self):\n",
    "        '''\n",
    "        A method for fetching all links from the webpage\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            self: object\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            links: list of all links in the given url\n",
    "        '''\n",
    "        links=[] # list to store all the links\n",
    "        try:\n",
    "            # fetching all <a> tags\n",
    "            self.a_tags = self.soup.find_all('a')\n",
    "        except:\n",
    "            print(\"Cannot find all <a> tags\")\n",
    "        \n",
    "        # iterating through each element in the list of <a> tags\n",
    "        for tag in self.a_tags:\n",
    "            items = str(tag).split(\" \")\n",
    "            for item in items:\n",
    "                if 'href' in item:\n",
    "                    link_item = item.split(\"=\")[1].split(\"\\\"\")[1]\n",
    "                    if 'http' in link_item or 'www' in link_item:\n",
    "                        links.append(link_item)\n",
    "        return links\n",
    "            \n",
    "    def get_all_image_links(self):\n",
    "        '''\n",
    "        A method for fetching all image links from the webpage\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            self: object\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            self.img_links: list of all image links in the given url\n",
    "        '''\n",
    "        self.img_links=[] # list to store all images\n",
    "        \n",
    "        # iterating through each element in the list of images\n",
    "        img_tags = self.soup.find_all('img')\n",
    "        for tag in img_tags:\n",
    "            items = str(tag).split(\" \")\n",
    "            for item in items:\n",
    "                if 'src' in item:\n",
    "                    link_item = item.split(\"=\")[1].split(\"\\\"\")[1]\n",
    "                    if 'http' in link_item or 'www' in link_item:\n",
    "                        self.img_links.append(link_item)\n",
    "        return self.img_links\n",
    "        \n",
    "    def download_images(self, image_list):\n",
    "        '''\n",
    "        A method for downloading images from the image links\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            self: object\n",
    "            image_list: list of image links\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            Images in the image links inside an 'images' directory within current\n",
    "            working directory.\n",
    "        '''\n",
    "        self.image_list = image_list\n",
    "        try:\n",
    "            # fetching today's date for directory name\n",
    "            day = self.datetime.date.today().day\n",
    "            month = self.datetime.date.today().month\n",
    "            year = self.datetime.date.today().year\n",
    "\n",
    "            date_ = str(day)+\"-\"+str(month)+\"-\"+str(year)+\"_\"\n",
    " \n",
    "            # creating a new directory to store images\n",
    "            if 'images' not in self.os.listdir():\n",
    "                self.os.mkdir(\"images\")\n",
    "                self.os.chdir(\"images\")\n",
    "            \n",
    "            self.os.mkdir(date_)\n",
    "            self.os.chdir(date_)\n",
    "            \n",
    "            #image number\n",
    "            img_no = 1\n",
    "                \n",
    "            # iterating through list of image links\n",
    "            for link in self.image_list:\n",
    "                # fetching image from webpage\n",
    "                img_response = self.requests.get(link)\n",
    "                \n",
    "                #file format\n",
    "                img_format = link.split(\".\")[-1]\n",
    "                \n",
    "                # creating a filename to store the image\n",
    "                filename = \"img\" + str(img_no) + \".\" + img_format\n",
    "                \n",
    "                # saving the image using the filename\n",
    "                with open(filename, \"wb+\") as f:\n",
    "                    f.write(img_response.content)\n",
    "                img_no += 1\n",
    "                \n",
    "            print(f\"{len(self.img_links)} images downloaded succesfully into 'images/{date_}' directory.\")    \n",
    "        except Exception as e:\n",
    "            print(\"Error while downloading images: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b4a5c",
   "metadata": {},
   "source": [
    "# Implementing static scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4664ac",
   "metadata": {},
   "source": [
    "Finally, we have made our scraper class. Its time that we now test our StaticSiteScraper. For that, I'm now moving on to craigslist.org's NewYork real estate page.\n",
    "\n",
    "Getting the url from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f9eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://newyork.craigslist.org/d/real-estate/search/rea\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e45595",
   "metadata": {},
   "source": [
    "Creating a **craig** object using our StaticSiteScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b41388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating class object\n",
    "craig = StaticSiteScraper(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63d391",
   "metadata": {},
   "source": [
    "Parsing the html contents and storing it in a variable called **page**.  \n",
    "\n",
    "We can use this variable for accessing any other html elements within this page if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing html contents\n",
    "page = craig.url_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec730a4",
   "metadata": {},
   "source": [
    "Fetching the title of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba05aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new york real estate - craigslist\n"
     ]
    }
   ],
   "source": [
    "# Fetching webpage title\n",
    "craig.get_title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b199b",
   "metadata": {},
   "source": [
    "Now, we will use **get_all_links** method in our StaticSiteScraper class to access all the links within this webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcec61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all links within the webpage\n",
    "craig.get_all_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f1be7",
   "metadata": {},
   "source": [
    "Similarly, we will now fetch the links of all the images in the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c6d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//www.craigslist.org/images/animated-spinny.gif',\n",
       " '//www.craigslist.org/images/animated-spinny.gif']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetching the links of all images within the webpage\n",
    "imagedata = craig.get_all_image_links()\n",
    "imagedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355ae27",
   "metadata": {},
   "source": [
    "Before proceeding to download all the images, make sure to check the following:\n",
    "\n",
    "**1. If we were able to fetch image links**\n",
    "    \n",
    "Why I'm saying this is because, almost all of the websites post their images within a \\<div\\> tag and may be within more tags inside the \\<div\\> tag. So, it will be very hard to fetch all the images in one go.\n",
    "\n",
    "Here, you will have to make use of the **page** variable we had created before to dig deeper into the tags where you can find the image link. From there, you have to fetch each images and store it into a list.\n",
    "    \n",
    "**2. If image links are in proper format**\n",
    "\n",
    "Most of the time, image links might not be in a proper format and we might need to rearrange them in proper format before passing to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eefc907b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.craigslist.org/images/animated-spinny.gif',\n",
       " 'http://www.craigslist.org/images/animated-spinny.gif']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formatting image list\n",
    "image_list = []\n",
    "for item in imagedata:\n",
    "    image_list.append(\"http:\"+item)\n",
    "\n",
    "image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38181231",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 images downloaded succesfully into 'images/16-11-2021_' directory.\n"
     ]
    }
   ],
   "source": [
    "# Downloading all images within the webpage into a 'images' directory\n",
    "craig.download_images(image_list=image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632fa05",
   "metadata": {},
   "source": [
    "With that, we have now created a webscraper that can fetch all the static content of a given url. Finally I hope you liked it. Feel free to post your valuable feedbacks and suggestions.\n",
    "\n",
    "You can contact me at: retinpkumar@gmail.com or visit my github profile at https://github.com/Retinpkumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e6a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
